{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3f728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#SKLearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "\n",
    "#Metrics\n",
    "import sklearn\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# class imbalance\n",
    "from imblearn.pipeline import make_pipeline as imb_make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# algorithms/models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model evaluation\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# performance check\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sparse to dense\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8400301c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>words_per_comment</th>\n",
       "      <th>variance_of_word_counts</th>\n",
       "      <th>http_per_comment</th>\n",
       "      <th>img_per_comment</th>\n",
       "      <th>qm_per_comment</th>\n",
       "      <th>excl_per_comment</th>\n",
       "      <th>ellipsis_per_comment</th>\n",
       "      <th>E</th>\n",
       "      <th>I</th>\n",
       "      <th>...</th>\n",
       "      <th>T</th>\n",
       "      <th>P</th>\n",
       "      <th>J</th>\n",
       "      <th>nouns_per_comment</th>\n",
       "      <th>adjs_per_comment</th>\n",
       "      <th>verbs_per_comment</th>\n",
       "      <th>prepositions_per_comment</th>\n",
       "      <th>interjections_per_comment</th>\n",
       "      <th>determiners_per_comment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>11.12</td>\n",
       "      <td>135.2900</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.18</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.166656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>23.40</td>\n",
       "      <td>187.4756</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.12</td>\n",
       "      <td>4.68</td>\n",
       "      <td>2.72</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.104312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>16.72</td>\n",
       "      <td>180.6900</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1.68</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.145745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>21.28</td>\n",
       "      <td>181.8324</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1.80</td>\n",
       "      <td>4.14</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.131263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>19.34</td>\n",
       "      <td>196.4576</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.80</td>\n",
       "      <td>4.42</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.075231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  posts  words_per_comment  \\\n",
       "type                                                                         \n",
       "INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...              11.12   \n",
       "ENTP  'I'm finding the lack of me in these posts ver...              23.40   \n",
       "INTP  'Good one  _____   https://www.youtube.com/wat...              16.72   \n",
       "INTJ  'Dear INTP,   I enjoyed our conversation the o...              21.28   \n",
       "ENTJ  'You're fired.|||That's another silly misconce...              19.34   \n",
       "\n",
       "      variance_of_word_counts  http_per_comment  img_per_comment  \\\n",
       "type                                                               \n",
       "INFJ                 135.2900              0.48             0.12   \n",
       "ENTP                 187.4756              0.20             0.02   \n",
       "INTP                 180.6900              0.10             0.00   \n",
       "INTJ                 181.8324              0.04             0.00   \n",
       "ENTJ                 196.4576              0.12             0.04   \n",
       "\n",
       "      qm_per_comment  excl_per_comment  ellipsis_per_comment  E  I  ...  T  P  \\\n",
       "type                                                                ...         \n",
       "INFJ            0.36              0.06                  0.30  0  1  ...  0  0   \n",
       "ENTP            0.10              0.00                  0.38  1  0  ...  1  1   \n",
       "INTP            0.24              0.08                  0.26  0  1  ...  1  1   \n",
       "INTJ            0.22              0.06                  0.52  0  1  ...  1  0   \n",
       "ENTJ            0.20              0.02                  0.42  1  0  ...  1  0   \n",
       "\n",
       "      J  nouns_per_comment  adjs_per_comment  verbs_per_comment  \\\n",
       "type                                                              \n",
       "INFJ  1               5.18              1.62               1.72   \n",
       "ENTP  0               3.98              2.12               4.68   \n",
       "INTP  0               2.70              1.68               3.10   \n",
       "INTJ  1               3.28              1.80               4.14   \n",
       "ENTJ  1               3.58              1.80               4.42   \n",
       "\n",
       "      prepositions_per_comment  interjections_per_comment  \\\n",
       "type                                                        \n",
       "INFJ                      0.54                       0.00   \n",
       "ENTP                      2.72                       0.04   \n",
       "INTP                      1.32                       0.08   \n",
       "INTJ                      1.88                       0.04   \n",
       "ENTJ                      1.78                       0.06   \n",
       "\n",
       "      determiners_per_comment  sentiment  \n",
       "type                                      \n",
       "INFJ                     1.06   0.166656  \n",
       "ENTP                     1.84   0.104312  \n",
       "INTP                     1.10   0.145745  \n",
       "INTJ                     1.86   0.131263  \n",
       "ENTJ                     1.60   0.075231  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path =Path(r'C:\\Users\\aanch\\OneDrive\\Desktop\\New Folder\\df_train.csv')\n",
    "df_train = pd.read_csv(df_path, index_col=[\"type\"])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91980af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_Extrovert</th>\n",
       "      <th>is_Sensing</th>\n",
       "      <th>is_Thinking</th>\n",
       "      <th>is_Judging</th>\n",
       "      <th>text</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>moment sportscenter top ten play prank life ch...</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>finding lack post alarming sex boring position...</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>good one course say know blessing curse absolu...</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dear enjoyed conversation day esoteric gabbing...</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fired another silly misconception approaching ...</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      is_Extrovert  is_Sensing  is_Thinking  is_Judging  \\\n",
       "type                                                      \n",
       "INFJ             0           0            0           1   \n",
       "ENTP             1           0            1           0   \n",
       "INTP             0           0            1           0   \n",
       "INTJ             0           0            1           1   \n",
       "ENTJ             1           0            1           1   \n",
       "\n",
       "                                                   text  Compound  Negative  \\\n",
       "type                                                                          \n",
       "INFJ  moment sportscenter top ten play prank life ch...    0.9924     0.132   \n",
       "ENTP  finding lack post alarming sex boring position...    0.9987     0.119   \n",
       "INTP  good one course say know blessing curse absolu...    0.9985     0.116   \n",
       "INTJ  dear enjoyed conversation day esoteric gabbing...    0.9985     0.087   \n",
       "ENTJ  fired another silly misconception approaching ...    0.9930     0.186   \n",
       "\n",
       "      Neutral  Positive  \n",
       "type                     \n",
       "INFJ    0.597     0.271  \n",
       "ENTP    0.598     0.284  \n",
       "INTP    0.581     0.303  \n",
       "INTJ    0.661     0.252  \n",
       "ENTJ    0.538     0.276  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_path = Path(r\"C:\\Users\\aanch\\OneDrive\\Desktop\\New folder\\df_combined.csv\")\n",
    "df_combined = pd.read_csv(combined_path, index_col=[\"type\"])\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88f1e8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>words_per_comment</th>\n",
       "      <th>variance_of_word_counts</th>\n",
       "      <th>http_per_comment</th>\n",
       "      <th>img_per_comment</th>\n",
       "      <th>qm_per_comment</th>\n",
       "      <th>excl_per_comment</th>\n",
       "      <th>ellipsis_per_comment</th>\n",
       "      <th>E</th>\n",
       "      <th>...</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>is_Extrovert</th>\n",
       "      <th>is_Sensing</th>\n",
       "      <th>is_Thinking</th>\n",
       "      <th>is_Judging</th>\n",
       "      <th>text</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>11.12</td>\n",
       "      <td>135.2900</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>moment sportscenter top ten play prank life ch...</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>23.40</td>\n",
       "      <td>187.4756</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104312</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>finding lack post alarming sex boring position...</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>16.72</td>\n",
       "      <td>180.6900</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145745</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>good one course say know blessing curse absolu...</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>21.28</td>\n",
       "      <td>181.8324</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131263</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dear enjoyed conversation day esoteric gabbing...</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>19.34</td>\n",
       "      <td>196.4576</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fired another silly misconception approaching ...</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  words_per_comment  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...              11.12   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...              23.40   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...              16.72   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...              21.28   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...              19.34   \n",
       "\n",
       "   variance_of_word_counts  http_per_comment  img_per_comment  qm_per_comment  \\\n",
       "0                 135.2900              0.48             0.12            0.36   \n",
       "1                 187.4756              0.20             0.02            0.10   \n",
       "2                 180.6900              0.10             0.00            0.24   \n",
       "3                 181.8324              0.04             0.00            0.22   \n",
       "4                 196.4576              0.12             0.04            0.20   \n",
       "\n",
       "   excl_per_comment  ellipsis_per_comment  E  ...  sentiment  is_Extrovert  \\\n",
       "0              0.06                  0.30  0  ...   0.166656             0   \n",
       "1              0.00                  0.38  1  ...   0.104312             1   \n",
       "2              0.08                  0.26  0  ...   0.145745             0   \n",
       "3              0.06                  0.52  0  ...   0.131263             0   \n",
       "4              0.02                  0.42  1  ...   0.075231             1   \n",
       "\n",
       "   is_Sensing  is_Thinking  is_Judging  \\\n",
       "0           0            0           1   \n",
       "1           0            1           0   \n",
       "2           0            1           0   \n",
       "3           0            1           1   \n",
       "4           0            1           1   \n",
       "\n",
       "                                                text  Compound  Negative  \\\n",
       "0  moment sportscenter top ten play prank life ch...    0.9924     0.132   \n",
       "1  finding lack post alarming sex boring position...    0.9987     0.119   \n",
       "2  good one course say know blessing curse absolu...    0.9985     0.116   \n",
       "3  dear enjoyed conversation day esoteric gabbing...    0.9985     0.087   \n",
       "4  fired another silly misconception approaching ...    0.9930     0.186   \n",
       "\n",
       "   Neutral  Positive  \n",
       "0    0.597     0.271  \n",
       "1    0.598     0.284  \n",
       "2    0.581     0.303  \n",
       "3    0.661     0.252  \n",
       "4    0.538     0.276  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat([df_train,df_combined], axis='columns')\n",
    "combined_df = combined_df.reset_index()\n",
    "combined_df = combined_df[combined_df[\"text\"].isnull()==False]\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86e215f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(r'C:\\Users\\aanch\\OneDrive\\Desktop\\New folder\\df_combined.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0834dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type',\n",
       " 'posts',\n",
       " 'words_per_comment',\n",
       " 'variance_of_word_counts',\n",
       " 'http_per_comment',\n",
       " 'img_per_comment',\n",
       " 'qm_per_comment',\n",
       " 'excl_per_comment',\n",
       " 'ellipsis_per_comment',\n",
       " 'E',\n",
       " 'I',\n",
       " 'S',\n",
       " 'N',\n",
       " 'F',\n",
       " 'T',\n",
       " 'P',\n",
       " 'J',\n",
       " 'nouns_per_comment',\n",
       " 'adjs_per_comment',\n",
       " 'verbs_per_comment',\n",
       " 'prepositions_per_comment',\n",
       " 'interjections_per_comment',\n",
       " 'determiners_per_comment',\n",
       " 'sentiment',\n",
       " 'is_Extrovert',\n",
       " 'is_Sensing',\n",
       " 'is_Thinking',\n",
       " 'is_Judging',\n",
       " 'text',\n",
       " 'Compound',\n",
       " 'Negative',\n",
       " 'Neutral',\n",
       " 'Positive']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(combined_df.columns.values)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50ff387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8674, 15)\n",
      "(8674, 4)\n"
     ]
    }
   ],
   "source": [
    "X = combined_df[\n",
    "    [\n",
    "        \"text\",\n",
    "        \"sentiment\",\n",
    "        \"nouns_per_comment\",\n",
    "        \"adjs_per_comment\",\n",
    "        \"verbs_per_comment\",\n",
    "        \"prepositions_per_comment\",\n",
    "        \"interjections_per_comment\",\n",
    "        \"determiners_per_comment\",\n",
    "        \"words_per_comment\",\n",
    "        \"variance_of_word_counts\",\n",
    "        \"http_per_comment\",\n",
    "        \"img_per_comment\",\n",
    "        \"qm_per_comment\",\n",
    "        \"excl_per_comment\",\n",
    "        \"ellipsis_per_comment\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# setting y to four target classes -> is_Extrovert, is_Sensing, is_Thinking, is_Judging\n",
    "y = combined_df.iloc[:, 24:28]\n",
    "\n",
    "# ensuring that X and y row count matches\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1f70a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = [\n",
    "    \"hey\",\"hello\",'enfps','entps','infps','infjs','ne','enfjs','intps','ni','entjs','intjs','estps','isfps','istps',\n",
    "    'isfjs','infps','infjs','se','enfps','si','intps','esfjs','intjs','entps','istjs','estps','ne','infps','infjs',\n",
    "    'intps','intjs','entps','enfps','nt','ti','istjs','infjs','infps','intjs','ni','intps','entps','ne','istps','enfps',\n",
    "    'isfjs','fi','fe','istjs','sx','ha','anime','type','perc','xd','hahaha','estjs','sp','lol','jawz','mbti','sj','nf','yo'\n",
    "    'hsp','hitler'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd20489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_n_scores = [\n",
    "#     \"text\",\n",
    "    \"sentiment\",\n",
    "    \"nouns_per_comment\",\n",
    "    \"adjs_per_comment\",\n",
    "    \"verbs_per_comment\",\n",
    "    \"prepositions_per_comment\",\n",
    "    \"interjections_per_comment\",\n",
    "    \"determiners_per_comment\",\n",
    "    \"words_per_comment\",\n",
    "    \"variance_of_word_counts\",\n",
    "    \"http_per_comment\",\n",
    "    \"img_per_comment\",\n",
    "    \"qm_per_comment\",\n",
    "    \"excl_per_comment\",\n",
    "    \"ellipsis_per_comment\"\n",
    "]\n",
    "\n",
    "# for selecting k best features from features other than words\n",
    "best_k_features = make_pipeline(MinMaxScaler(), SelectKBest(f_classif, k=10))\n",
    "\n",
    "# setting up preprocessing for TF-IDF vectorizer\n",
    "preprocesser_tf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(min_df=25, max_df=0.85, stop_words=additional_stopwords),\n",
    "            \"text\",\n",
    "        ),\n",
    "        (\"selectbest\", best_k_features, counts_n_scores),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# setting up preprocessing for COUNT vectorizer\n",
    "preprocesser_ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"ct_vect\",\n",
    "            CountVectorizer(min_df=25, max_df=0.85, stop_words=additional_stopwords),\n",
    "            \"text\",\n",
    "        ),\n",
    "        (\"selectbest\", best_k_features, counts_n_scores),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5db791bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_type = {\n",
    "    \"is_Extrovert\": \"Extrovert vs Introvert\",\n",
    "    \"is_Sensing\": \"Sensing vs Intuition\",\n",
    "    \"is_Thinking\": \"Thinking vs Feeling\",\n",
    "    \"is_Judging\": \"Judging vs Perceiving\",\n",
    "}\n",
    "\n",
    "# function to build the model for predicting each of the 4 target classes\n",
    "def build_model(model, X, target, vectorizer_name):\n",
    "\n",
    "    for col in target.columns:\n",
    "\n",
    "        print(f\"\\n{mbti_type[col]}\")\n",
    "        target = y[col]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        # model training\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # y_hat\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # y_probability\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # precision recall score\n",
    "        average_precision = average_precision_score(y_test, y_proba)\n",
    "\n",
    "        # model evaluation\n",
    "        print(\n",
    "            f\"Geometric Mean Score: {geometric_mean_score(y_test, y_pred, average='weighted'):.2f}\"\n",
    "        )\n",
    "        print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_proba):.2f}\")\n",
    "        print(f\"Average Precision-Recall Score: {average_precision:.2f}\")\n",
    "        print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fff4c5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imb_make_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imb_make_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tfidf_logistic_regression = imb_make_pipeline(\n",
    "    preprocesser_tf, RandomUnderSampler(), LogisticRegressionCV()\n",
    ")\n",
    "build_model(tfidf_logistic_regression, X, y, \"tfidf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
